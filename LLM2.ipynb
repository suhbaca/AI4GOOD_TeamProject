{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhbaca/AI4GOOD_TeamProject/blob/suh/LLM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTWCJnUlvNug",
        "outputId": "5ebbef83-00f7-4468-cbf1-3a94a79db569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "#access huggingface\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EmoKQ-si6lvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Chat:\n",
        "\n",
        "  def __init__(self, prompt) :\n",
        "    self.prompt= prompt\n",
        "    #load model\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",quantization_config=bnb_config)\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    #create a transformer pipeline for text generation\n",
        "    self.pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "\n",
        "    model=self.model,\n",
        "\n",
        "   tokenizer=self.tokenizer,\n",
        "  torch_dtype=torch.float16,\n",
        "\n",
        "    device_map=\"auto\",\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "#given a prompt it will add the specific instructions that will give the output in the correct format\n",
        "#the current additions use few-shot prompt\n",
        "  def add_template(self):\n",
        "\n",
        "    #this is the main instruction of what is being done\n",
        "    instruction= 'The user is trying to find a word that matches the description in the prompt. The answer is 5 possible words that the prompt is trying to describe. The following are examples of Prompt-Answer format.\\n'  # eg Given the context provided, answer the question\n",
        "\n",
        "    #this is the few shot examples being passed\n",
        "    few_shot = \"Prompt: it’s a type of bird, kind of rare. They’re very colorful, with long tails, and they can mimic sounds. They live in tropical areas\\n Answer:  Hummingbird, Parrot, Finch, Toucan, Macaw\\n Prompt: It’s a type of bird, very colorful, and it can mimic human speech. People sometimes keep them as pets. They live in tropical areas.\\nAnswer: Parrot, Macaw, Cockatoo, Canary, Finch\\nPrompt: I need the...you know...the thing for writing\\nAnswer: Pen, Pencil, Quill, Fountain Pen, Crayon\\nPrompt : I want to go to the...um...place where we get groceries\\n  Answer: Store, Supermarket, Market, Grocery, Shop\\nPrompt : \"\n",
        "\n",
        "    #this is the specific instruction given to the model to generate the words\n",
        "    system_prompt = 'Now it is your turn to give the answer to the last prompt. Give 30 possible words being described instead of just 5 like in the examples. It is important that you give 30 words. It is very important that you only provide the final output without any additional comments or remarks or notes.'\n",
        "\n",
        "    chat_prompt= instruction + few_shot + self.prompt + system_prompt\n",
        "    return chat_prompt\n",
        "\n",
        "  def clean_generated(self, gen):\n",
        "    #first strip the generated answer from 'Answer:'\n",
        "    gen.replace('Answer: ', '')\n",
        "    #next split the text by \\n\n",
        "    x = gen.split('\\n')\n",
        "    #it is possible the current line is in the format : word1, word2, word3, ...-> split on ,\n",
        "    possible_words = x[0].split(',')\n",
        "\n",
        "    return possible_words\n",
        "\n",
        "#this make will take the refined prompt and pass it to the LLM\n",
        "  def get_suggestions(self):\n",
        "    sequences = self.pipeline(\n",
        "    self.add_template(),\n",
        "\n",
        "    do_sample=True,\n",
        "\n",
        "    top_k=20,\n",
        "\n",
        "    num_return_sequences=1,\n",
        "\n",
        "    eos_token_id=self.tokenizer.eos_token_id,\n",
        "\n",
        "    max_length=800,\n",
        "\n",
        "    temperature= 0.3,\n",
        "\n",
        "    return_full_text= False\n",
        "\n",
        "    )\n",
        "    #sugg= self.clean_generated(sequences['generated_text'])\n",
        "\n",
        "    #return sugg\n",
        "    return sequences['generated_text']\n"
      ],
      "metadata": {
        "id": "kpGcM7LC5xhM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "2dcKvQ_W7l9Z",
        "outputId": "d0811dcf-0d37-4a50-d476-dfe3112e1071",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### wrapper function\n",
        "#creates an instance of chat\n",
        "#runs the llm tp generate an answer\n",
        "# cleans the generated answer into a list and return the list\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM,  BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "def get_list(prompt):\n",
        "  chat = Chat(prompt)\n",
        "  return chat.get_suggestions()\n",
        "\n",
        "\n",
        "print(get_list(\"What's the word for...uh...the machine that cleans clothes?\"))\n"
      ],
      "metadata": {
        "id": "jRslPrNjMclZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}